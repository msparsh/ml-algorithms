{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55fa8fd",
   "metadata": {},
   "source": [
    "# Learning Implementation for Stochastic Linear Regression\n",
    "with regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22de211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea21577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b171ba73",
   "metadata": {},
   "source": [
    "# Observations:\n",
    "* Reacts better to large bias than BatchGD\n",
    "* If features have noncompareable sizes then bigger feature gets more weight\n",
    "* Runs 1 step after last print  \n",
    "\n",
    "\n",
    "# Query:\n",
    "* Should bias update be multiplied by lr\n",
    "* What diff does dividing by m makes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195ef49",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda341eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def added_cost(Lambda,W,m):\n",
    "    \"\"\"Returns added cost of regularization.\n",
    "    \"\"\"\n",
    "    \n",
    "    return (np.sum(W**2)) * Lambda/(2*m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8150986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for alpha and its multiplication with the reg term\n",
    "def single_update(i,X,y,y_,W,b,alpha,Lambda):\n",
    "    \"\"\"Returns W,b after single update of ith data point.\n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    res = y[i]-y_[i]\n",
    "        \n",
    "    dJ_dW = np.dot(res,X[i])  - Lambda*W\n",
    "    dJ_db = res.mean()\n",
    "    \n",
    "    W += dJ_dW*alpha/m\n",
    "    b += (y[i]-y_[i])*alpha\n",
    "\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1820eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_LinearRegression(X,y, iterations = 100,alpha = 0.000001,Lambda=0.0001, output_limit=10):\n",
    "    \"\"\"Returns W,b after training lr using stochastic gradient descent with regularization applied.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    iterations: int, default=100\n",
    "        Number of complete iterations through X\n",
    "        \n",
    "    alpha: float, default=0.000001\n",
    "        Constant Learning Rate\n",
    "    \n",
    "    Lambda: float, default=0.0001\n",
    "        Rate for l2 Regularization\n",
    "        \n",
    "    output_limit: int, default=10\n",
    "        Number of iterations to show\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    W: numpy.ndarray\n",
    "        The optimized weights.\n",
    "    b: numpy.longdouble\n",
    "        The optimized itercept.\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_limit<=0:\n",
    "        print(\"Choose natural output limit!\")\n",
    "        return None, None\n",
    "    m,n = X.shape\n",
    "    W = np.zeros(n)\n",
    "    b = np.float128(0)\n",
    "    \n",
    "    try:\n",
    "        for k in range(iterations+1):\n",
    "            y_ = np.dot(X,W) +b\n",
    "            \n",
    "            if k % (iterations//output_limit) == 0:\n",
    "                print(f\"({k//(iterations//output_limit)}/{output_limit}) > Iteration: {k}\",\n",
    "                      f\"Cost: {mean_squared_error(y,y_) + added_cost(Lambda,W,m) }\",\n",
    "                      f\"   Weights: {W}\",\n",
    "                      f\"Bias: {b:.4f}\"\n",
    "                     )\n",
    "                \n",
    "            for i in range(m):\n",
    "                W,b = single_update(i,X,y,y_,W,b,alpha,Lambda)\n",
    "                y_ = np.dot(X,W) + b\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\nTerminated! Returned: Weights: {W}, Bias: {b}\")\n",
    "        return W,b\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eba465",
   "metadata": {},
   "source": [
    "## Running Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a62bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(50,3)\n",
    "y = 5*X[:,0] + 11*X[:,1] + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "963ae765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/15) > Iteration: 0 Cost: 191.90536503310975    Weights: [0. 0. 0.] Bias: 0.0000\n",
      "(1/15) > Iteration: 66 Cost: 0.9953610503431713    Weights: [3.1967 7.7181 0.136 ] Bias: 7.4594\n",
      "(2/15) > Iteration: 132 Cost: 0.10336241976122358    Weights: [ 4.3433 10.0057  0.0448] Bias: 5.7822\n",
      "(3/15) > Iteration: 198 Cost: 0.011415146594860922    Weights: [4.7609e+00 1.0694e+01 8.1587e-03] Bias: 5.2560\n",
      "(4/15) > Iteration: 264 Cost: 0.0015311334988053506    Weights: [ 4.9105e+00  1.0900e+01 -7.1853e-04] Bias: 5.0892\n",
      "(5/15) > Iteration: 330 Cost: 0.0003695470986607692    Weights: [ 4.9634e+00  1.0961e+01 -1.4799e-03] Bias: 5.0361\n",
      "(6/15) > Iteration: 396 Cost: 0.00020603884385756658    Weights: [ 4.9820e+00  1.0978e+01 -7.8539e-04] Bias: 5.0191\n",
      "(7/15) > Iteration: 462 Cost: 0.00017594916936634246    Weights: [ 4.9885e+00  1.0983e+01 -1.9530e-04] Bias: 5.0137\n",
      "(8/15) > Iteration: 528 Cost: 0.00016883708161496378    Weights: [4.9907e+00 1.0985e+01 1.2906e-04] Bias: 5.0120\n",
      "(9/15) > Iteration: 594 Cost: 0.00016688967511047185    Weights: [4.9915e+00 1.0985e+01 2.8120e-04] Bias: 5.0115\n",
      "(10/15) > Iteration: 660 Cost: 0.00016632356915123334    Weights: [4.9918e+00 1.0985e+01 3.4681e-04] Bias: 5.0113\n",
      "(11/15) > Iteration: 726 Cost: 0.00016615652892953907    Weights: [4.9919e+00 1.0985e+01 3.7365e-04] Bias: 5.0113\n",
      "(12/15) > Iteration: 792 Cost: 0.00016610750909184794    Weights: [4.9919e+00 1.0985e+01 3.8425e-04] Bias: 5.0112\n",
      "(13/15) > Iteration: 858 Cost: 0.00016609335954538963    Weights: [4.9919e+00 1.0985e+01 3.8832e-04] Bias: 5.0112\n",
      "(14/15) > Iteration: 924 Cost: 0.00016608937978476287    Weights: [4.9919e+00 1.0985e+01 3.8986e-04] Bias: 5.0112\n",
      "(15/15) > Iteration: 990 Cost: 0.0001660883024330511    Weights: [4.9919e+00 1.0985e+01 3.9043e-04] Bias: 5.0112\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(precision=4):\n",
    "    W,b = SGD_LinearRegression(X,y, 1000,0.25,output_limit=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea7fb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using sklearn SGDRegressor\n",
    "m = make_pipeline(\n",
    "    SGDRegressor(\n",
    "    max_iter=1000,\n",
    "    n_iter_no_change=100, \n",
    "    penalty='l2', \n",
    "    verbose=1, \n",
    "    learning_rate='constant', eta0=0.01, \n",
    "    tol=0.001\n",
    "    )\n",
    ")\n",
    "    \n",
    "m.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c72848f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.94568172, 10.92544288, -0.01962791])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0].coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
