{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58cc05bf",
   "metadata": {},
   "source": [
    "# Logistic Regression (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60a231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a72321",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"Logistic Regression implementation.\n",
    "\n",
    "    This class provides functionalities to perform logistic regression. Check the fit method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"To add globals if required to make model persistent.\"\"\"\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def sigmoid_dot(self, X, W, b):\n",
    "        \"\"\"Returns sigmoid of (W.X + b)\"\"\"\n",
    "        \n",
    "        return 1 / (1 + np.exp(-(np.dot(X, W) + b)))\n",
    "    \n",
    "    def update(self, X, y, y_, W, b, alpha, m):\n",
    "        \"\"\"Updates W, b stochastically for each datapoint.\"\"\"\n",
    "        \n",
    "        res = y-y_\n",
    "        for i in range(m):\n",
    "\n",
    "            dJ_dW = np.dot(res[i],X[i]) / m\n",
    "            dJ_db = np.mean(res)\n",
    "            \n",
    "            W += alpha * dJ_dW\n",
    "            b += alpha * dJ_db\n",
    "        return W, b \n",
    "    \n",
    "    def cost(self, y, y_):\n",
    "        \"\"\"Returns logistic cost between predicted values and true labels.\"\"\"\n",
    "        \n",
    "        m = y.shape[0]\n",
    "        c = 0\n",
    "        \n",
    "        for i in range(m):\n",
    "            c += y[i] * np.log(y_[i]) + (1 - y[i]) * np.log(1 - y_[i])\n",
    "        return c / (-m)\n",
    "    \n",
    "    def fit(self, X, y, iterations=1000, alpha=0.000001):\n",
    "        \"\"\"\n",
    "        Fits the logistic regression model to the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Input data of shape (m, n).\n",
    "        y : np.ndarray\n",
    "            True labels of shape (m,).\n",
    "        iterations : int, optional\n",
    "            Number of iterations for training. Default is 1000.\n",
    "        alpha : float, optional\n",
    "            Learning rate. Default is 0.000001\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            Trained model parameters W and b.\n",
    "        \"\"\"\n",
    "        \n",
    "        m, n = X.shape\n",
    "        W = np.random.rand(n)\n",
    "        b = 0\n",
    "        \n",
    "        for k in range(iterations):\n",
    "            y_ = self.sigmoid_dot(X, W, b)\n",
    "            W, b = self.update(X, y, y_, W, b, alpha,m)\n",
    "            print(f\"Iteration: {k}\",\n",
    "                  f\"Cost: {self.cost(y, y_)}\",\n",
    "                  f\"Acc: {accuracy_score(y, y_.round())}\")\n",
    "        return W, b\n",
    "    \n",
    "    def predict(self, X, W, b):\n",
    "        \"\"\"Generates predictions for input data X using trained model parameters W and b.\n",
    "        \n",
    "        Returns rounded predictions. Might need to fix.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = self.sigmoid_dot(X, W, b)\n",
    "        return s.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a4513",
   "metadata": {},
   "source": [
    "# Dataset Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b00a7",
   "metadata": {},
   "source": [
    "Multilabel Dataset. Using 2 targets at a time as Model is Binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4569395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddfcd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iteration: 0 Cost: 0.8313986194881866 Acc: 0.5\n",
      "Iteration: 1 Cost: 0.740713426560121 Acc: 0.5\n",
      "Iteration: 2 Cost: 0.6747030224020663 Acc: 0.5\n",
      "Iteration: 3 Cost: 0.6290495110395917 Acc: 0.5\n",
      "Iteration: 4 Cost: 0.5987444207086395 Acc: 0.5\n",
      "Iteration: 5 Cost: 0.5792204604081066 Acc: 0.6\n",
      "Iteration: 6 Cost: 0.5668885455955076 Acc: 0.65\n",
      "Iteration: 7 Cost: 0.559187254150925 Acc: 0.78\n",
      "Iteration: 8 Cost: 0.554398574804787 Acc: 0.84\n",
      "Iteration: 9 Cost: 0.5514147191299946 Acc: 0.84\n",
      "Iteration: 10 Cost: 0.5495387922929182 Acc: 0.92\n",
      "Iteration: 11 Cost: 0.548339048576372 Acc: 0.92\n",
      "Iteration: 12 Cost: 0.5475505219125477 Acc: 0.92\n",
      "Iteration: 13 Cost: 0.5470115399148036 Acc: 0.92\n",
      "Iteration: 14 Cost: 0.5466238057138489 Acc: 0.94\n",
      "Iteration: 15 Cost: 0.546327684379735 Acc: 0.94\n",
      "Iteration: 16 Cost: 0.5460870443580582 Acc: 0.94\n",
      "Iteration: 17 Cost: 0.545880011885911 Acc: 0.94\n",
      "Iteration: 18 Cost: 0.5456933511504259 Acc: 0.94\n",
      "Iteration: 19 Cost: 0.5455190554367858 Acc: 0.94\n",
      "Iteration: 20 Cost: 0.5453522820318457 Acc: 0.94\n",
      "Iteration: 21 Cost: 0.5451901021462107 Acc: 0.94\n",
      "Iteration: 22 Cost: 0.545030744548943 Acc: 0.94\n",
      "Iteration: 23 Cost: 0.5448731380516159 Acc: 0.94\n",
      "Iteration: 24 Cost: 0.544716634806315 Acc: 0.94\n",
      "Iteration: 25 Cost: 0.5445608429724593 Acc: 0.94\n",
      "Iteration: 26 Cost: 0.5444055255272325 Acc: 0.94\n",
      "Iteration: 27 Cost: 0.5442505390747009 Acc: 0.94\n",
      "Iteration: 28 Cost: 0.5440957968422925 Acc: 0.94\n",
      "Iteration: 29 Cost: 0.5439412463033366 Acc: 0.94\n",
      "Iteration: 30 Cost: 0.5437868556440913 Acc: 0.94\n",
      "Iteration: 31 Cost: 0.543632605579219 Acc: 0.94\n",
      "Iteration: 32 Cost: 0.5434784844017034 Acc: 0.94\n",
      "Iteration: 33 Cost: 0.5433244849888433 Acc: 0.94\n",
      "Iteration: 34 Cost: 0.5431706029912677 Acc: 0.94\n",
      "Iteration: 35 Cost: 0.5430168357374743 Acc: 0.94\n",
      "Iteration: 36 Cost: 0.5428631815711424 Acc: 0.94\n",
      "Iteration: 37 Cost: 0.542709639450235 Acc: 0.94\n",
      "Iteration: 38 Cost: 0.5425562087044541 Acc: 0.94\n",
      "Iteration: 39 Cost: 0.5424028888884881 Acc: 0.94\n",
      "Iteration: 40 Cost: 0.5422496796932227 Acc: 0.94\n",
      "Iteration: 41 Cost: 0.542096580892007 Acc: 0.94\n",
      "Iteration: 42 Cost: 0.5419435923081407 Acc: 0.94\n",
      "Iteration: 43 Cost: 0.5417907137951985 Acc: 0.94\n",
      "Iteration: 44 Cost: 0.5416379452251171 Acc: 0.94\n",
      "Iteration: 45 Cost: 0.5414852864809856 Acc: 0.94\n",
      "Iteration: 46 Cost: 0.5413327374526865 Acc: 0.94\n",
      "Iteration: 47 Cost: 0.5411802980342485 Acc: 0.94\n",
      "Iteration: 48 Cost: 0.5410279681222512 Acc: 0.94\n",
      "Iteration: 49 Cost: 0.5408757476148535 Acc: 0.94\n",
      "Iteration: 50 Cost: 0.5407236364112077 Acc: 0.94\n",
      "Iteration: 51 Cost: 0.5405716344111076 Acc: 0.94\n",
      "Iteration: 52 Cost: 0.5404197415147659 Acc: 0.94\n",
      "Iteration: 53 Cost: 0.5402679576226906 Acc: 0.94\n",
      "Iteration: 54 Cost: 0.5401162826356021 Acc: 0.94\n",
      "Iteration: 55 Cost: 0.5399647164543864 Acc: 0.94\n",
      "Iteration: 56 Cost: 0.5398132589800663 Acc: 0.94\n",
      "Iteration: 57 Cost: 0.5396619101137816 Acc: 0.94\n",
      "Iteration: 58 Cost: 0.5395106697567821 Acc: 0.94\n",
      "Iteration: 59 Cost: 0.5393595378104162 Acc: 0.94\n",
      "Iteration: 60 Cost: 0.5392085141761318 Acc: 0.94\n",
      "Iteration: 61 Cost: 0.5390575987554711 Acc: 0.94\n",
      "Iteration: 62 Cost: 0.538906791450068 Acc: 0.94\n",
      "Iteration: 63 Cost: 0.5387560921616483 Acc: 0.94\n",
      "Iteration: 64 Cost: 0.5386055007920336 Acc: 0.94\n",
      "Iteration: 65 Cost: 0.5384550172431325 Acc: 0.94\n",
      "Iteration: 66 Cost: 0.5383046414169447 Acc: 0.94\n",
      "Iteration: 67 Cost: 0.5381543732155628 Acc: 0.94\n",
      "Iteration: 68 Cost: 0.5380042125411687 Acc: 0.94\n",
      "Iteration: 69 Cost: 0.5378541592960356 Acc: 0.94\n",
      "Iteration: 70 Cost: 0.5377042133825257 Acc: 0.94\n",
      "Iteration: 71 Cost: 0.5375543747030931 Acc: 0.94\n",
      "Iteration: 72 Cost: 0.5374046431602819 Acc: 0.94\n",
      "Iteration: 73 Cost: 0.5372550186567252 Acc: 0.94\n",
      "Iteration: 74 Cost: 0.5371055010951484 Acc: 0.94\n",
      "Iteration: 75 Cost: 0.5369560903783634 Acc: 0.94\n",
      "Iteration: 76 Cost: 0.536806786409277 Acc: 0.94\n",
      "Iteration: 77 Cost: 0.5366575890908827 Acc: 0.94\n",
      "Iteration: 78 Cost: 0.5365084983262642 Acc: 0.94\n",
      "Iteration: 79 Cost: 0.5363595140185972 Acc: 0.94\n",
      "Iteration: 80 Cost: 0.5362106360711452 Acc: 0.94\n",
      "Iteration: 81 Cost: 0.5360618643872609 Acc: 0.94\n",
      "Iteration: 82 Cost: 0.5359131988703889 Acc: 0.94\n",
      "Iteration: 83 Cost: 0.5357646394240619 Acc: 0.94\n",
      "Iteration: 84 Cost: 0.5356161859519045 Acc: 0.94\n",
      "Iteration: 85 Cost: 0.5354678383576263 Acc: 0.94\n",
      "Iteration: 86 Cost: 0.5353195965450321 Acc: 0.94\n",
      "Iteration: 87 Cost: 0.5351714604180118 Acc: 0.94\n",
      "Iteration: 88 Cost: 0.5350234298805474 Acc: 0.94\n",
      "Iteration: 89 Cost: 0.5348755048367082 Acc: 0.94\n",
      "Iteration: 90 Cost: 0.5347276851906535 Acc: 0.94\n",
      "Iteration: 91 Cost: 0.5345799708466324 Acc: 0.94\n",
      "Iteration: 92 Cost: 0.5344323617089843 Acc: 0.94\n",
      "Iteration: 93 Cost: 0.534284857682136 Acc: 0.94\n",
      "Iteration: 94 Cost: 0.5341374586706022 Acc: 0.94\n",
      "Iteration: 95 Cost: 0.5339901645789898 Acc: 0.94\n",
      "Iteration: 96 Cost: 0.5338429753119911 Acc: 0.94\n",
      "Iteration: 97 Cost: 0.5336958907743913 Acc: 0.94\n",
      "Iteration: 98 Cost: 0.5335489108710626 Acc: 0.94\n",
      "Iteration: 99 Cost: 0.5334020355069647 Acc: 0.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True, False, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = load_iris()[\"data\"][50:150,3:4]\n",
    "y = load_iris()[\"target\"][50:150] - 1\n",
    "print(y)\n",
    "m = LogisticRegression()\n",
    "W,b = m.fit(X,y,100, 0.009)\n",
    "m.predict(X, W,b) == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c357bef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Iteration: 0 Cost: 0.5752742996475233 Acc: 0.5\n",
      "Iteration: 1 Cost: 0.5655645988310147 Acc: 0.55\n",
      "Iteration: 2 Cost: 0.5593814298072648 Acc: 0.91\n",
      "Iteration: 3 Cost: 0.5554068566854082 Acc: 0.98\n",
      "Iteration: 4 Cost: 0.5527997305433344 Acc: 0.98\n",
      "Iteration: 5 Cost: 0.5510332828343762 Acc: 0.99\n",
      "Iteration: 6 Cost: 0.5497814715972722 Acc: 1.0\n",
      "Iteration: 7 Cost: 0.5488439438453815 Acc: 1.0\n",
      "Iteration: 8 Cost: 0.5480981242486379 Acc: 1.0\n",
      "Iteration: 9 Cost: 0.5474692224266665 Acc: 1.0\n",
      "Iteration: 10 Cost: 0.5469116781289426 Acc: 1.0\n",
      "Iteration: 11 Cost: 0.5463977639551746 Acc: 1.0\n",
      "Iteration: 12 Cost: 0.5459106147099115 Acc: 1.0\n",
      "Iteration: 13 Cost: 0.5454399752127558 Acc: 1.0\n",
      "Iteration: 14 Cost: 0.544979610470847 Acc: 1.0\n",
      "Iteration: 15 Cost: 0.544525729798766 Acc: 1.0\n",
      "Iteration: 16 Cost: 0.5440760284360386 Acc: 1.0\n",
      "Iteration: 17 Cost: 0.5436291048775272 Acc: 1.0\n",
      "Iteration: 18 Cost: 0.5431841066817178 Acc: 1.0\n",
      "Iteration: 19 Cost: 0.5427405151789648 Acc: 1.0\n",
      "Iteration: 20 Cost: 0.5422980146091401 Acc: 1.0\n",
      "Iteration: 21 Cost: 0.5418564125759423 Acc: 1.0\n",
      "Iteration: 22 Cost: 0.5414155916916813 Acc: 1.0\n",
      "Iteration: 23 Cost: 0.5409754801803636 Acc: 1.0\n",
      "Iteration: 24 Cost: 0.5405360340047376 Acc: 1.0\n",
      "Iteration: 25 Cost: 0.5400972259987749 Acc: 1.0\n",
      "Iteration: 26 Cost: 0.5396590392591307 Acc: 1.0\n",
      "Iteration: 27 Cost: 0.5392214631260843 Acc: 1.0\n",
      "Iteration: 28 Cost: 0.5387844907390543 Acc: 1.0\n",
      "Iteration: 29 Cost: 0.5383481175496428 Acc: 1.0\n",
      "Iteration: 30 Cost: 0.5379123404170497 Acc: 1.0\n",
      "Iteration: 31 Cost: 0.5374771570576973 Acc: 1.0\n",
      "Iteration: 32 Cost: 0.5370425657103342 Acc: 1.0\n",
      "Iteration: 33 Cost: 0.5366085649322312 Acc: 1.0\n",
      "Iteration: 34 Cost: 0.536175153475136 Acc: 1.0\n",
      "Iteration: 35 Cost: 0.5357423302097692 Acc: 1.0\n",
      "Iteration: 36 Cost: 0.5353100940798542 Acc: 1.0\n",
      "Iteration: 37 Cost: 0.5348784440741284 Acc: 1.0\n",
      "Iteration: 38 Cost: 0.5344473792092974 Acc: 1.0\n",
      "Iteration: 39 Cost: 0.5340168985196556 Acc: 1.0\n",
      "Iteration: 40 Cost: 0.5335870010507597 Acc: 1.0\n",
      "Iteration: 41 Cost: 0.5331576858555745 Acc: 1.0\n",
      "Iteration: 42 Cost: 0.5327289519921268 Acc: 1.0\n",
      "Iteration: 43 Cost: 0.5323007985220658 Acc: 1.0\n",
      "Iteration: 44 Cost: 0.5318732245098043 Acc: 1.0\n",
      "Iteration: 45 Cost: 0.5314462290219675 Acc: 1.0\n",
      "Iteration: 46 Cost: 0.5310198111270842 Acc: 1.0\n",
      "Iteration: 47 Cost: 0.530593969895372 Acc: 1.0\n",
      "Iteration: 48 Cost: 0.5301687043986296 Acc: 1.0\n",
      "Iteration: 49 Cost: 0.529744013710155 Acc: 1.0\n",
      "Iteration: 50 Cost: 0.5293198969046976 Acc: 1.0\n",
      "Iteration: 51 Cost: 0.5288963530584361 Acc: 1.0\n",
      "Iteration: 52 Cost: 0.5284733812489573 Acc: 1.0\n",
      "Iteration: 53 Cost: 0.5280509805552412 Acc: 1.0\n",
      "Iteration: 54 Cost: 0.527629150057662 Acc: 1.0\n",
      "Iteration: 55 Cost: 0.5272078888379788 Acc: 1.0\n",
      "Iteration: 56 Cost: 0.5267871959793318 Acc: 1.0\n",
      "Iteration: 57 Cost: 0.5263670705662417 Acc: 1.0\n",
      "Iteration: 58 Cost: 0.5259475116846091 Acc: 1.0\n",
      "Iteration: 59 Cost: 0.5255285184217092 Acc: 1.0\n",
      "Iteration: 60 Cost: 0.525110089866195 Acc: 1.0\n",
      "Iteration: 61 Cost: 0.524692225108095 Acc: 1.0\n",
      "Iteration: 62 Cost: 0.5242749232388123 Acc: 1.0\n",
      "Iteration: 63 Cost: 0.5238581833511227 Acc: 1.0\n",
      "Iteration: 64 Cost: 0.5234420045391744 Acc: 1.0\n",
      "Iteration: 65 Cost: 0.5230263858984897 Acc: 1.0\n",
      "Iteration: 66 Cost: 0.5226113265259604 Acc: 1.0\n",
      "Iteration: 67 Cost: 0.5221968255198497 Acc: 1.0\n",
      "Iteration: 68 Cost: 0.5217828819797897 Acc: 1.0\n",
      "Iteration: 69 Cost: 0.5213694950067844 Acc: 1.0\n",
      "Iteration: 70 Cost: 0.5209566637032015 Acc: 1.0\n",
      "Iteration: 71 Cost: 0.5205443871727805 Acc: 1.0\n",
      "Iteration: 72 Cost: 0.5201326645206241 Acc: 1.0\n",
      "Iteration: 73 Cost: 0.5197214948532036 Acc: 1.0\n",
      "Iteration: 74 Cost: 0.519310877278354 Acc: 1.0\n",
      "Iteration: 75 Cost: 0.5189008109052735 Acc: 1.0\n",
      "Iteration: 76 Cost: 0.5184912948445282 Acc: 1.0\n",
      "Iteration: 77 Cost: 0.5180823282080432 Acc: 1.0\n",
      "Iteration: 78 Cost: 0.5176739101091049 Acc: 1.0\n",
      "Iteration: 79 Cost: 0.5172660396623637 Acc: 1.0\n",
      "Iteration: 80 Cost: 0.5168587159838287 Acc: 1.0\n",
      "Iteration: 81 Cost: 0.5164519381908682 Acc: 1.0\n",
      "Iteration: 82 Cost: 0.5160457054022084 Acc: 1.0\n",
      "Iteration: 83 Cost: 0.5156400167379348 Acc: 1.0\n",
      "Iteration: 84 Cost: 0.5152348713194885 Acc: 1.0\n",
      "Iteration: 85 Cost: 0.5148302682696673 Acc: 1.0\n",
      "Iteration: 86 Cost: 0.514426206712623 Acc: 1.0\n",
      "Iteration: 87 Cost: 0.5140226857738606 Acc: 1.0\n",
      "Iteration: 88 Cost: 0.5136197045802425 Acc: 1.0\n",
      "Iteration: 89 Cost: 0.5132172622599793 Acc: 1.0\n",
      "Iteration: 90 Cost: 0.512815357942635 Acc: 1.0\n",
      "Iteration: 91 Cost: 0.5124139907591231 Acc: 1.0\n",
      "Iteration: 92 Cost: 0.5120131598417054 Acc: 1.0\n",
      "Iteration: 93 Cost: 0.5116128643239962 Acc: 1.0\n",
      "Iteration: 94 Cost: 0.5112131033409544 Acc: 1.0\n",
      "Iteration: 95 Cost: 0.510813876028886 Acc: 1.0\n",
      "Iteration: 96 Cost: 0.5104151815254439 Acc: 1.0\n",
      "Iteration: 97 Cost: 0.5100170189696245 Acc: 1.0\n",
      "Iteration: 98 Cost: 0.5096193875017682 Acc: 1.0\n",
      "Iteration: 99 Cost: 0.5092222862635584 Acc: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = load_iris()[\"data\"][0:100,3:4]\n",
    "y = load_iris()[\"target\"][0:100]\n",
    "\n",
    "print(y)\n",
    "m = LogisticRegression()\n",
    "W,b = m.fit(X,y,100, 0.009)\n",
    "m.predict(X, W,b) == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d79ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(1000, 5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af09cc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Cost: 0.38673834831778126 Acc: 0.861\n",
      "Iteration: 1 Cost: 0.385626385076859 Acc: 0.863\n",
      "Iteration: 2 Cost: 0.3848657388848308 Acc: 0.866\n",
      "Iteration: 3 Cost: 0.38416288721611075 Acc: 0.867\n",
      "Iteration: 4 Cost: 0.38347218653910836 Acc: 0.867\n",
      "Iteration: 5 Cost: 0.3827865529878571 Acc: 0.867\n",
      "Iteration: 6 Cost: 0.38210487515365227 Acc: 0.867\n",
      "Iteration: 7 Cost: 0.3814269598140023 Acc: 0.867\n",
      "Iteration: 8 Cost: 0.38075275417049637 Acc: 0.867\n",
      "Iteration: 9 Cost: 0.38008222689749516 Acc: 0.869\n",
      "Iteration: 10 Cost: 0.37941535011788013 Acc: 0.869\n",
      "Iteration: 11 Cost: 0.37875209667624826 Acc: 0.869\n",
      "Iteration: 12 Cost: 0.37809243973265527 Acc: 0.87\n",
      "Iteration: 13 Cost: 0.3774363526988877 Acc: 0.87\n",
      "Iteration: 14 Cost: 0.3767838092274813 Acc: 0.871\n",
      "Iteration: 15 Cost: 0.3761347832078828 Acc: 0.872\n",
      "Iteration: 16 Cost: 0.37548924876391804 Acc: 0.873\n",
      "Iteration: 17 Cost: 0.37484718025138425 Acc: 0.874\n",
      "Iteration: 18 Cost: 0.3742085522557093 Acc: 0.874\n",
      "Iteration: 19 Cost: 0.3735733395896406 Acc: 0.874\n",
      "Iteration: 20 Cost: 0.37294151729095265 Acc: 0.874\n",
      "Iteration: 21 Cost: 0.37231306062017544 Acc: 0.873\n",
      "Iteration: 22 Cost: 0.3716879450583567 Acc: 0.874\n",
      "Iteration: 23 Cost: 0.3710661463048404 Acc: 0.875\n",
      "Iteration: 24 Cost: 0.3704476402750713 Acc: 0.875\n",
      "Iteration: 25 Cost: 0.3698324030984206 Acc: 0.876\n",
      "Iteration: 26 Cost: 0.3692204111160338 Acc: 0.877\n",
      "Iteration: 27 Cost: 0.36861164087870846 Acc: 0.879\n",
      "Iteration: 28 Cost: 0.36800606914478123 Acc: 0.879\n",
      "Iteration: 29 Cost: 0.3674036728780516 Acc: 0.879\n",
      "Iteration: 30 Cost: 0.36680442924571527 Acc: 0.879\n",
      "Iteration: 31 Cost: 0.3662083156163269 Acc: 0.879\n",
      "Iteration: 32 Cost: 0.3656153095577851 Acc: 0.879\n",
      "Iteration: 33 Cost: 0.36502538883532865 Acc: 0.88\n",
      "Iteration: 34 Cost: 0.3644385314095677 Acc: 0.881\n",
      "Iteration: 35 Cost: 0.3638547154345262 Acc: 0.883\n",
      "Iteration: 36 Cost: 0.36327391925570757 Acc: 0.883\n",
      "Iteration: 37 Cost: 0.36269612140818386 Acc: 0.883\n",
      "Iteration: 38 Cost: 0.36212130061469777 Acc: 0.884\n",
      "Iteration: 39 Cost: 0.36154943578379933 Acc: 0.884\n",
      "Iteration: 40 Cost: 0.36098050600797743 Acc: 0.885\n",
      "Iteration: 41 Cost: 0.360414490561845 Acc: 0.885\n",
      "Iteration: 42 Cost: 0.35985136890030683 Acc: 0.885\n",
      "Iteration: 43 Cost: 0.3592911206567803 Acc: 0.885\n",
      "Iteration: 44 Cost: 0.35873372564141087 Acc: 0.885\n",
      "Iteration: 45 Cost: 0.35817916383931697 Acc: 0.886\n",
      "Iteration: 46 Cost: 0.35762741540885434 Acc: 0.886\n",
      "Iteration: 47 Cost: 0.3570784606798937 Acc: 0.886\n",
      "Iteration: 48 Cost: 0.3565322801521243 Acc: 0.886\n",
      "Iteration: 49 Cost: 0.3559888544933654 Acc: 0.886\n",
      "Iteration: 50 Cost: 0.3554481645379026 Acc: 0.886\n",
      "Iteration: 51 Cost: 0.3549101912848458 Acc: 0.886\n",
      "Iteration: 52 Cost: 0.35437491589649184 Acc: 0.886\n",
      "Iteration: 53 Cost: 0.35384231969671986 Acc: 0.888\n",
      "Iteration: 54 Cost: 0.3533123841693872 Acc: 0.889\n",
      "Iteration: 55 Cost: 0.3527850909567618 Acc: 0.889\n",
      "Iteration: 56 Cost: 0.35226042185794987 Acc: 0.889\n",
      "Iteration: 57 Cost: 0.3517383588273615 Acc: 0.89\n",
      "Iteration: 58 Cost: 0.3512188839731764 Acc: 0.89\n",
      "Iteration: 59 Cost: 0.35070197955583327 Acc: 0.891\n",
      "Iteration: 60 Cost: 0.35018762798653413 Acc: 0.891\n",
      "Iteration: 61 Cost: 0.349675811825767 Acc: 0.891\n",
      "Iteration: 62 Cost: 0.3491665137818385 Acc: 0.891\n",
      "Iteration: 63 Cost: 0.3486597167094265 Acc: 0.892\n",
      "Iteration: 64 Cost: 0.34815540360814934 Acc: 0.893\n",
      "Iteration: 65 Cost: 0.34765355762114675 Acc: 0.894\n",
      "Iteration: 66 Cost: 0.3471541620336761 Acc: 0.896\n",
      "Iteration: 67 Cost: 0.3466572002717272 Acc: 0.896\n",
      "Iteration: 68 Cost: 0.34616265590064804 Acc: 0.896\n",
      "Iteration: 69 Cost: 0.3456705126237878 Acc: 0.896\n",
      "Iteration: 70 Cost: 0.34518075428115347 Acc: 0.896\n",
      "Iteration: 71 Cost: 0.3446933648480771 Acc: 0.897\n",
      "Iteration: 72 Cost: 0.34420832843390936 Acc: 0.898\n",
      "Iteration: 73 Cost: 0.34372562928070416 Acc: 0.898\n",
      "Iteration: 74 Cost: 0.3432452517619473 Acc: 0.898\n",
      "Iteration: 75 Cost: 0.3427671803812746 Acc: 0.898\n",
      "Iteration: 76 Cost: 0.34229139977120654 Acc: 0.899\n",
      "Iteration: 77 Cost: 0.3418178946919163 Acc: 0.899\n",
      "Iteration: 78 Cost: 0.3413466500299823 Acc: 0.899\n",
      "Iteration: 79 Cost: 0.34087765079717586 Acc: 0.9\n",
      "Iteration: 80 Cost: 0.34041088212925336 Acc: 0.9\n",
      "Iteration: 81 Cost: 0.3399463292847563 Acc: 0.901\n",
      "Iteration: 82 Cost: 0.3394839776438389 Acc: 0.901\n",
      "Iteration: 83 Cost: 0.33902381270708976 Acc: 0.902\n",
      "Iteration: 84 Cost: 0.33856582009438013 Acc: 0.903\n",
      "Iteration: 85 Cost: 0.3381099855437204 Acc: 0.903\n",
      "Iteration: 86 Cost: 0.33765629491012067 Acc: 0.903\n",
      "Iteration: 87 Cost: 0.3372047341644824 Acc: 0.903\n",
      "Iteration: 88 Cost: 0.3367552893924771 Acc: 0.903\n",
      "Iteration: 89 Cost: 0.33630794679345705 Acc: 0.903\n",
      "Iteration: 90 Cost: 0.33586269267937036 Acc: 0.903\n",
      "Iteration: 91 Cost: 0.33541951347368293 Acc: 0.903\n",
      "Iteration: 92 Cost: 0.3349783957103204 Acc: 0.903\n",
      "Iteration: 93 Cost: 0.3345393260326135 Acc: 0.903\n",
      "Iteration: 94 Cost: 0.3341022911922619 Acc: 0.903\n",
      "Iteration: 95 Cost: 0.3336672780483015 Acc: 0.903\n",
      "Iteration: 96 Cost: 0.3332342735660895 Acc: 0.903\n",
      "Iteration: 97 Cost: 0.33280326481629463 Acc: 0.903\n",
      "Iteration: 98 Cost: 0.3323742389739012 Acc: 0.903\n",
      "Iteration: 99 Cost: 0.33194718331722245 Acc: 0.904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True, False,  True,  True, False,\n",
       "        True,  True,  True, False,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True, False, False,  True,  True, False,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True, False,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False, False,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True, False, False, False,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True, False,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "       False,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True, False,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True, False,  True,  True,  True, False,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression()\n",
    "W,b = m.fit(X,y,100, 0.008)\n",
    "m.predict(X, W,b) == y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
